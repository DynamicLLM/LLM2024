{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON+u5AxUjYai/BVgeBssnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DynamicLLM/LLM2024/blob/main/src/sample-ai-agent/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program implements a Retrieval-Augmented Generation (RAG) system that combines semantic search and dynamic text generation. It uses tinyDB to cache user queries and their corresponding responses, leveraging sentence embeddings from SentenceTransformer to find semantically similar questions based on cosine similarity. If a similar question is found, the cached response is returned. If no match is found, the program generates a new response dynamically using OpenAI's GPT model (gpt-4o-mini). The generated response is then stored in the tinyDB database for future use. This hybrid approach optimizes response accuracy while reducing redundant computations, making it ideal for chatbots, FAQ systems, and knowledge-base applications."
      ],
      "metadata": {
        "id": "Y2ijMlf8-lq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tinydb"
      ],
      "metadata": {
        "id": "8JKwkDXzwcoS",
        "outputId": "558d3b2d-f771-4dae-dc16-651a0a387e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tinydb\n",
            "  Downloading tinydb-4.8.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading tinydb-4.8.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: tinydb\n",
            "Successfully installed tinydb-4.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "from tinydb import TinyDB, Query\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "# Cache settings\n",
        "CACHE_EXPIRATION = 3600  # 1 hour in seconds\n",
        "SIMILARITY_THRESHOLD = 0.8  # Threshold for similarity\n",
        "DB_FILE = \"cache.json\"  # TinyDB file\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=\"sk-proj-***A\"\n",
        ")\n",
        "\n",
        "# Initialize TinyDB\n",
        "db = TinyDB(DB_FILE)\n",
        "query = Query()\n",
        "\n",
        "def set_cached_response(question, response, embedding):\n",
        "    \"\"\"Stores the question, response, and embedding in TinyDB.\"\"\"\n",
        "    try:\n",
        "        timestamp = time.time()\n",
        "        db.upsert({\n",
        "            'question': question,\n",
        "            'response': response,\n",
        "            'embedding': json.dumps(embedding.tolist()),  # Convert embedding to JSON string\n",
        "            'timestamp': timestamp\n",
        "        }, query.question == question)\n",
        "        print(f\"Cached response for: {question}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set cache: {str(e)}\")\n",
        "\n",
        "def find_similar_question(question, embedding):\n",
        "    \"\"\"Searches for a semantically similar question in TinyDB.\"\"\"\n",
        "    try:\n",
        "        current_time = time.time()\n",
        "        records = db.all()\n",
        "\n",
        "        for record in records:\n",
        "            if current_time - record['timestamp'] < CACHE_EXPIRATION:\n",
        "                cached_embedding = np.array(json.loads(record['embedding']))\n",
        "                similarity = 1 - cosine(embedding, cached_embedding)\n",
        "                if similarity > SIMILARITY_THRESHOLD:\n",
        "                    print(f\"Found similar question with similarity: {similarity}\")\n",
        "                    return record['question'], record['response']\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search cache: {str(e)}\")\n",
        "    return None, None\n",
        "\n",
        "def generate_response(query):\n",
        "    \"\"\"Generates a response using OpenAI's GPT model.\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            store=True,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate response: {str(e)}\")\n",
        "        return \"I'm sorry, I couldn't generate a response at this time.\"\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding for the input text.\"\"\"\n",
        "    return embedding_model.encode([text])[0]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    user_question = input(\"Enter your question: \")\n",
        "    user_embedding = get_embedding(user_question)\n",
        "\n",
        "    # Search for similar questions in the cache\n",
        "    cached_question, cached_response = find_similar_question(user_question, user_embedding)\n",
        "\n",
        "    if cached_response:\n",
        "        print(f\"Found similar question: {cached_question}\")\n",
        "        print(f\"Cached response: {cached_response}\")\n",
        "    else:\n",
        "        print(\"No similar question found. Generating a new response...\")\n",
        "        generated_response = generate_response(user_question)\n",
        "        print(f\"Generated response: {generated_response}\")\n",
        "\n",
        "        # Cache the generated response\n",
        "        set_cached_response(user_question, generated_response, user_embedding)\n"
      ],
      "metadata": {
        "id": "IcPKNMDQwie4",
        "outputId": "4225d3c7-541e-4498-a5ae-10d48bfafbca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: oK\n",
            "No similar question found. Generating a new response...\n",
            "Generated response: Hello! How can I assist you today?\n",
            "Cached response for: oK\n"
          ]
        }
      ]
    }
  ]
}